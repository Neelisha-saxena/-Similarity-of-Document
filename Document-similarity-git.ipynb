{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine_similarity\n",
    "<b>algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import  stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path1='para_test.txt'\n",
    "f=open(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words=['run','ran','giving','factionally','augmentation','worked','I','me','you','your','my']\n",
    "stemmed_tokens=[stemmer.stem(t) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run',\n",
       " 'ran',\n",
       " 'give',\n",
       " 'faction',\n",
       " 'augment',\n",
       " 'work',\n",
       " 'I',\n",
       " 'me',\n",
       " 'you',\n",
       " 'your',\n",
       " 'my']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(file):\n",
    "    f=open(file)\n",
    "    raw= f.read()\n",
    "    f.close()\n",
    "    tokens=word_tokenize(raw)\n",
    "    \n",
    "    words=[w.lower() for w in tokens]\n",
    "    stemmer = PorterStemmer()#removing running run\n",
    "    stemmed_tokens=[stemmer.stem(t) for t in words]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filtered=[w for w in stemmed_tokens if w not in stop_words]\n",
    "    count=nltk.defaultdict(int)\n",
    "    for word in filtered:\n",
    "        count[word]+=1\n",
    "    print(file )\n",
    "    print(count )\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarity(dict1 ,dict2):\n",
    "    all_words=[]\n",
    "    for key in dict1:\n",
    "        all_words.append(key)\n",
    "    for key in dict2:\n",
    "        all_words.append(key)\n",
    "        \n",
    "    len_word=len(all_words)\n",
    "    v1=np.zeros(len_word,dtype=np.int)\n",
    "    v2=np.zeros(len_word ,dtype=np.int)\n",
    "    i=0\n",
    "    for (key) in all_words:\n",
    "        v1[i]=dict1.get(key,0)\n",
    "        v2[i]=dict2.get(key,0)\n",
    "        i=i+1\n",
    "    return cos_sim(v1,v2)\n",
    "\n",
    "#is poore ka summary...[a:1 , b:2 , C:3] and [a:2 ,d:5]....[1,2,3,0] and [2,0,0,5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(v1,v2):\n",
    "    dot_product=np.dot(v1,v2)\n",
    "    norm_v1=np.linalg.norm(v1)\n",
    "    norm_v2=np.linalg.norm(v2)\n",
    "     \n",
    "    return (dot_product/(norm_v1*norm_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path1='para_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "para_test.txt\n",
      "defaultdict(<class 'int'>, {'tell': 2, 'often': 1, 'miss': 1, 'support': 1, 'start': 1, 'word': 1, 'idea': 1, 'show': 1, 'statement': 1, 'easier': 1, 'sometim': 1, ';': 1, 'four': 1, 'multipl': 1, 'longer': 1, 'put': 1, '(': 2, 'five': 1, 'thesi': 1, 'use': 1, 'eight': 1, 'paragraph': 9, 'make': 2, 'central': 1, 'thi': 2, ')': 2, 'pilcrow': 1, 'space': 1, 'organ': 1, 'made': 1, ',': 5, 'indent': 1, '¶': 1, 'begin': 3, 'contain': 1, 'essay': 3, 'end': 1, 'claim': 1, 'topic': 2, 'one': 1, '.': 8, 'sentenc': 5, 'mani': 1, 'form': 2, 'group': 2, 'usual': 3, 'write': 1, 'mark': 1, 'anoth': 1, 'line': 1, 'togeth': 1, 'reader': 1})\n",
      "para_test.txt\n",
      "defaultdict(<class 'int'>, {'tell': 2, 'often': 1, 'miss': 1, 'support': 1, 'start': 1, 'word': 1, 'idea': 1, 'show': 1, 'statement': 1, 'easier': 1, 'sometim': 1, ';': 1, 'four': 1, 'multipl': 1, 'longer': 1, 'put': 1, '(': 2, 'five': 1, 'thesi': 1, 'use': 1, 'eight': 1, 'paragraph': 9, 'make': 2, 'central': 1, 'thi': 2, ')': 2, 'pilcrow': 1, 'space': 1, 'organ': 1, 'made': 1, ',': 5, 'indent': 1, '¶': 1, 'begin': 3, 'contain': 1, 'essay': 3, 'end': 1, 'claim': 1, 'topic': 2, 'one': 1, '.': 8, 'sentenc': 5, 'mani': 1, 'form': 2, 'group': 2, 'usual': 3, 'write': 1, 'mark': 1, 'anoth': 1, 'line': 1, 'togeth': 1, 'reader': 1})\n",
      "similarity between file1 and file 1=  100.0 percent\n"
     ]
    }
   ],
   "source": [
    "dict1=process(path1)\n",
    "dict2=process(path1)\n",
    "print(\"similarity between file1 and file 1= \",get_similarity(dict1,dict2)*100,'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path2='christmas2.txt'\n",
    "path3='christmas1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: Christmas is an annual festival celebrated in remembrance of the birth of Jesus Christ. It is a major Christian Festival. Though in some places, Christmas is also celebrated by Non-Christians.\r\n",
      "\r\n",
      "Why do we celebrate Christmas? The birthday of Jesus Christ, the great founder of the Christian faith, falls on 25th of December. Christians believe Jesus Christ to be the son and messenger of God. He is the light of the world. Christians believe that if they follow Jesus, then he will remove all the darkness of their life with his light. Hence, we celebrate Christmas Festival on this day.\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat christmas2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christmas is one of the most popular festivals worldwide. It is celebrated on 25th December every year, but the celebrations start well in advance. It is celebrated in the honour of the lord Jesus Christ as he was born on this day to mother Mary. Jesus lived an exemplary life and taught everyone to follow the path of goodness; he taught people to believe in the almighty and perform good deeds. The life led by Jesus is an example that must be followed by everyone for attaining salvation and for living in harmony on this earth.\r\n"
     ]
    }
   ],
   "source": [
    "! cat christmas1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christmas2.txt\n",
      "defaultdict(<class 'int'>, {'introduct': 1, 'day': 1, 'god': 1, 'birth': 1, 'christ': 3, ':': 1, 'jesu': 4, ',': 5, 'decemb': 1, 'messeng': 1, '?': 1, 'non-christian': 1, 'annual': 1, 'remov': 1, 'remembr': 1, 'life': 1, 'thi': 1, 'major': 1, 'light': 2, 'though': 1, 'place': 1, 'world': 1, 'henc': 1, 'also': 1, 'great': 1, 'fall': 1, 'follow': 1, 'christma': 4, 'celebr': 4, '25th': 1, 'believ': 2, 'founder': 1, 'hi': 1, 'faith': 1, 'festiv': 3, 'whi': 1, 'son': 1, 'christian': 4, '.': 8, 'birthday': 1, 'dark': 1})\n",
      "christmas1.txt\n",
      "defaultdict(<class 'int'>, {'decemb': 1, 'deed': 1, 'day': 1, 'mother': 1, ',': 1, 'start': 1, 'christma': 1, 'celebr': 3, ';': 1, 'advanc': 1, 'year': 1, 'earth': 1, 'follow': 2, 'path': 1, 'exemplari': 1, 'led': 1, 'live': 2, 'festiv': 1, 'honour': 1, 'peopl': 1, 'taught': 2, 'must': 1, 'everi': 1, 'popular': 1, 'thi': 2, 'christ': 1, 'good': 2, 'salvat': 1, 'well': 1, 'wa': 1, 'harmoni': 1, 'worldwid': 1, 'attain': 1, 'everyon': 2, 'one': 1, 'life': 2, 'mari': 1, 'born': 1, 'exampl': 1, 'lord': 1, 'believ': 1, '25th': 1, 'jesu': 3, '.': 5, 'perform': 1, 'almighti': 1})\n",
      "similarity between file2 and file 3=  71.2906230943 percent\n"
     ]
    }
   ],
   "source": [
    "dict3=process(path2)\n",
    "dict2=process(path3)\n",
    "print(\"similarity between file2 and file 3= \",get_similarity(dict2,dict3)*100,'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### direct packages for cosine similarity between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine scores ==>  [[ 1.          0.2456432   0.25301941]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "f = open(path1)\n",
    "doc1 = f.read() \n",
    "f = open(path2)\n",
    "doc2 =f.read() \n",
    "f = open(path3)\n",
    "doc3 =  f.read() \n",
    "\n",
    "train_set = [doc1,  doc2, doc3]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_set)  #finds the tfidf score with normalization\n",
    "print( \"cosine scores ==> \",cosine_similarity(tfidf_matrix_train[0:1], tfidf_matrix_train))  #here the first element of tfidf_matrix_train is matched with other three elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction: Christmas is an annual festival celebrated in remembrance of the birth of Jesus Christ. It is a major Christian Festival. Though in some places, Christmas is also celebrated by Non-Christians.\\n\\nWhy do we celebrate Christmas? The birthday of Jesus Christ, the great founder of the Christian faith, falls on 25th of December. Christians believe Jesus Christ to be the son and messenger of God. He is the light of the world. Christians believe that if they follow Jesus, then he will remove all the darkness of their life with his light. Hence, we celebrate Christmas Festival on this day.\\n\\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A paragraph is a group of words put together to form a group that is usually longer than a sentence. Paragraphs are often made up of many sentences. They are usually between four to eight sentences. Paragraphs can begin with an indentation (about five spaces), or by missing a line out, and then starting again; this makes telling when one paragraph ends and another begins easier.\\nIn most organized forms of writing, such as essays, paragraphs contain a topic sentence . This topic sentence of the paragraph tells the reader what the paragraph will be about. Essays usually have multiple paragraphs that make claims to support a thesis statement, which is the central idea of the essay.\\nA pilcrow mark (¶) is sometimes used to show where a paragraph begins.\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 =doc1\n",
    "d2 =doc2\n",
    "d3 = doc3\n",
    "documents = [d1, d2, d3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, string, numpy\n",
    "#nltk.download('punkt') # first-time use only  used for removing the puntuation\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def StemTokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def StemNormalize(text):\n",
    "    return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 43: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 60: None,\n",
       " 61: None,\n",
       " 62: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 94: None,\n",
       " 95: None,\n",
       " 96: None,\n",
       " 123: None,\n",
       " 124: None,\n",
       " 125: None,\n",
       " 126: None}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "     return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "     return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x97 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 108 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "LemVectorizer.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exemplary': 28, 'telling': 85, 'christmas': 16, 'start': 79, 'writing': 94, 'word': 91, 'harmony': 41, 'remove': 74, 'statement': 81, 'attaining': 4, 'celebrated': 11, 'year': 95, 'earth': 23, 'birth': 7, 'follow': 32, 'end': 25, 'path': 66, 'christian': 15, 'make': 56, 'taught': 83, '25th': 0, 'sentence': 76, 'central': 13, 'remembrance': 73, 'darkness': 19, 'pilcrow': 69, 'space': 78, 'great': 39, 'starting': 80, 'almighty': 2, 'son': 77, 'wa': 90, 'jesus': 46, 'multiple': 62, 'topic': 87, 'example': 27, 'mary': 58, 'place': 70, 'easier': 24, 'form': 34, 'group': 40, 'born': 9, 'december': 21, 'faith': 29, 'world': 92, 'line': 50, 'perform': 68, 'tell': 84, 'deed': 22, 'organized': 64, 'god': 36, 'celebrate': 10, 'usually': 89, 'mother': 61, 'support': 82, 'indentation': 44, 'popular': 71, 'followed': 33, 'worldwide': 93, 'idea': 43, 'introduction': 45, 'lived': 51, 'celebration': 12, 'nonchristians': 63, 'longer': 53, 'messenger': 59, 'day': 20, 'advance': 1, 'led': 47, 'used': 88, 'paragraph': 65, 'honour': 42, 'life': 48, 'festival': 31, 'birthday': 8, 'christ': 14, 'good': 37, 'begin': 5, 'goodness': 38, 'annual': 3, 'contain': 18, 'essay': 26, 'missing': 60, 'living': 52, 'major': 55, 'light': 49, 'claim': 17, '¶': 96, 'fall': 30, 'people': 67, 'lord': 54, 'mark': 57, 'founder': 35, 'salvation': 75, 'thesis': 86, 'believe': 6, 'reader': 72}\n"
     ]
    }
   ],
   "source": [
    "print(LemVectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 3 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 3 0 0 0 0 0 0 0 2 0 0\n",
      "  0 0 0 2 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 2 1 0 0 1 0 1 0 1 9 0 0 0 1 0 0 1 0\n",
      "  0 0 5 0 1 0 1 1 1 0 1 1 1 2 1 3 0 1 0 0 1 0 1]\n",
      " [1 0 0 1 0 0 2 1 1 0 2 2 0 0 3 4 4 0 0 1 1 1 0 0 0 0 0 0 0 1 1 3 1 0 0 1 1\n",
      "  0 0 1 0 0 0 0 0 1 4 0 1 2 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1\n",
      "  1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [1 1 1 0 1 0 1 0 0 1 0 2 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0\n",
      "  1 1 0 0 1 1 0 0 0 3 1 2 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0\n",
      "  0 1 0 0 0 1 0 0 0 2 0 0 0 0 0 0 1 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "tf_matrix = LemVectorizer.transform(documents).toarray()\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.28768207  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.28768207  1.69314718  1.69314718  1.69314718  1.69314718  1.28768207\n",
      "  1.69314718  1.69314718  1.28768207  1.69314718  1.28768207  1.69314718\n",
      "  1.69314718  1.69314718  1.28768207  1.28768207  1.69314718  1.69314718\n",
      "  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.28768207  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.69314718  1.69314718  1.69314718  1.28768207  1.69314718\n",
      "  1.28768207  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "print( tfidfTran.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The idf for terms that appear in one document: 1.916290731874155\n",
      "The idf for terms that appear in two documents: 1.5108256237659907\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def idf(n,df):\n",
    "    result = math.log((n+1.0)/(df+1.0)) + 1\n",
    "    return result\n",
    "print( \"The idf for terms that appear in one document: \" + str(idf(4,1)))\n",
    "print(\"The idf for terms that appear in two documents: \" + str(idf(4,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidfTran.transform(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.        ]\n",
      " [ 0.          1.          0.32231202]\n",
      " [ 0.          0.32231202  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "print (cos_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.32231202],\n",
       "       [ 0.        ,  0.32231202,  1.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()\n",
    "cos_similarity(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[1,2,3,2,6]\n",
    "b=[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
